#!/usr/bin/env python3
"""
Script de Benchmark Programado
===============================
Script wrapper para ejecuciones programadas del benchmark.
Carga la configuraciÃ³n de servidores y ejecuta el benchmark diario.

Uso:
    python scheduled_benchmark.py
    python scheduled_benchmark.py --config benchmark_config_servers.json
"""

import sys
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from logging_manager import get_log_manager
from daily_benchmark import DailyBenchmarkRunner


async def main():
    """Ejecutar benchmark programado"""
    parser = argparse.ArgumentParser(
        description="Benchmark Programado para Servidores Remotos"
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='benchmark_config_servers.json',
        help='Archivo de configuraciÃ³n (default: benchmark_config_servers.json)'
    )
    
    parser.add_argument(
        '--log-dir',
        type=str,
        default='.logs_scheduled',
        help='Directorio de logs (default: .logs_scheduled)'
    )
    
    args = parser.parse_args()
    
    # Verificar que existe el archivo de configuraciÃ³n
    if not Path(args.config).exists():
        print(f"âŒ Error: No se encontrÃ³ el archivo de configuraciÃ³n: {args.config}")
        print(f"ğŸ’¡ AsegÃºrate de que el archivo existe en el directorio actual")
        sys.exit(1)
    
    # Crear instancia del runner
    print(f"ğŸš€ Iniciando benchmark programado...")
    print(f"ğŸ“… Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âš™ï¸  ConfiguraciÃ³n: {args.config}")
    print(f"ğŸ“ Logs: {args.log_dir}")
    print("-" * 60)
    
    runner = DailyBenchmarkRunner(
        config_file=args.config,
        log_dir=args.log_dir
    )
    
    # Ejecutar benchmark
    success, result = await runner.run()
    
    if success:
        print("\n" + "=" * 60)
        print("âœ… BENCHMARK COMPLETADO EXITOSAMENTE")
        print("=" * 60)
        print(f"ğŸ“Š Tests ejecutados: {result.get('total_tests_executed', 0)}")
        print(f"ğŸŒ Entornos probados: {', '.join(result.get('environments_tested', []))}")
        print(f"ğŸ“ Endpoints probados: {len(result.get('endpoints_tested', []))}")
        print(f"â±ï¸  DuraciÃ³n: {result.get('execution_duration_seconds', 0):.1f} segundos")
        print(f"ğŸ“ˆ RPS Promedio: {result.get('avg_rps', 0):.2f}")
        print(f"âš¡ Latencia Promedio: {result.get('avg_latency_ms', 0):.2f} ms")
        print(f"âŒ Tasa de Error: {result.get('error_rate_percent', 0):.2f}%")
        print("=" * 60)
        sys.exit(0)
    else:
        print("\n" + "=" * 60)
        print("âŒ BENCHMARK FALLÃ“")
        print("=" * 60)
        print(f"ğŸ’¥ Error: {result.get('error', 'Error desconocido')}")
        print(f"ğŸ“ Revisa los logs en: {args.log_dir}")
        print("=" * 60)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
